# =============================================================================
# Environment Variables for Batch Processing and vLLM Inference
# =============================================================================
# Copy this file to .env and fill in your API keys
# Usage: source .env (or set in your shell profile)
# =============================================================================

# -----------------------------------------------------------------------------
# Cloud API Keys (required for batch processing notebooks)
# -----------------------------------------------------------------------------

# Anthropic Claude API
# Get your key at: https://console.anthropic.com/
ANTHROPIC_API_KEY=your-anthropic-api-key

# OpenAI API
# Get your key at: https://platform.openai.com/api-keys
OPENAI_API_KEY=your-openai-api-key

# Google Gemini API
# Get your key at: https://aistudio.google.com/apikey
GEMINI_API_KEY=your-gemini-api-key

# -----------------------------------------------------------------------------
# Azure OpenAI (optional - only if using Azure deployment)
# -----------------------------------------------------------------------------

# AZURE_OPENAI_API_KEY=your-azure-key
# AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com/

# -----------------------------------------------------------------------------
# HuggingFace (for vLLM model downloads)
# -----------------------------------------------------------------------------

# HuggingFace token for gated models
# Get your token at: https://huggingface.co/settings/tokens
# HF_TOKEN=your-huggingface-token

# Cache directory for downloaded models (optional)
# HF_HOME=/path/to/huggingface/cache

# -----------------------------------------------------------------------------
# SLURM Configuration (for vLLM inference - optional)
# -----------------------------------------------------------------------------
# These can be overridden in vllm_inference.sbatch

# GPU_TYPE=a40
# GPU_COUNT=4
# SLURM_PARTITION=gpu
# SLURM_QOS=normal
