# Chunk-by-Chunk Iterative Checklist Extraction

This pipeline extracts 26 checklist items from legal case documents by processing them chunk-by-chunk. Each document is split into 16K-token chunks, and checklist items are iteratively updated as new chunks are processed.

## Overview

**Key Features:**
- Processes documents in 16K-token chunks to handle long legal documents
- Extracts all 26 checklist items with evidence citations
- Uses vLLM for efficient batch inference on local GPUs
- Supports Qwen3 and GPT-OSS models with thinking mode
- Checkpoint support for resuming interrupted runs

---

## Workflow

### Step 1: Data Preparation

The Jupyter notebook processes raw case data into chunked prompts:

```bash
jupyter notebook create_data_for_chunk_by_chunk_pipeline.ipynb
```

**Input**: `data/full_case_data/{dataset}.json` (full case documents)

**Output**: `data/{dataset}.json` with structure:
```json
{
  "keys": [["case_id", "document_title"], ...],
  "chunks": [["chunk1", "chunk2", ...], ...],
  "chunks_tokens": [[token_count1, token_count2, ...], ...]
}
```

### Step 2: Batch Inference

Run vLLM inference to extract checklist items:

```bash
./submit_vllm_inference_jobs.sh
```

**Output**: `results/{model_name}/{dataset}_thinking_{true|false}.json`

---

## Processing Logic

The chunk-by-chunk algorithm processes all cases and items in parallel across chunks:

```
For each global chunk index i = 0, 1, 2, ... max_chunks:
    1. Identify active cases (cases that have chunk i)
    2. For each active case:
       - For each of 26 checklist items:
         - Generate prompt with: item description, current state, chunk text
    3. Batch process: 26 x (active cases) prompts via vLLM
    4. Parse responses and update checklist states
    5. Save checkpoint

Cases with fewer chunks finish early and are skipped in later iterations.
```

**Example**: With 10 cases where 8 have 3 chunks and 2 have 5 chunks:
- Chunk 1: Process 10 cases x 26 items = 260 prompts
- Chunk 2: Process 10 cases x 26 items = 260 prompts
- Chunk 3: Process 10 cases x 26 items = 260 prompts
- Chunk 4: Process 2 cases x 26 items = 52 prompts
- Chunk 5: Process 2 cases x 26 items = 52 prompts

The LLM stays loaded throughout all chunk iterations (no reinitialization) for efficiency.

---

## Files

| File | Description |
|------|-------------|
| `create_data_for_chunk_by_chunk_pipeline.ipynb` | Prepare chunked data from raw cases |
| `vllm_inference.py` | Main inference pipeline (1300+ lines) |
| `submit_vllm_inference_jobs.sh` | SLURM job submission orchestrator |
| `vllm_inference.sbatch` | SLURM batch configuration (4x A40 GPUs) |
| `data/` | Input chunked data (generated by notebook) |
| `results/` | Output extraction results |
| `checkpoints/` | Intermediate states for resuming |

---

## Quick Start

### 1. Prepare Data

```bash
# Open the notebook and run all cells
jupyter notebook create_data_for_chunk_by_chunk_pipeline.ipynb
```

Configure in the notebook:
- `input_file_name`: Dataset name (e.g., "20_human_eval_cases")
- `order_by_date`: Whether to sort documents chronologically
- `max_chunk_tokens`: Token limit per chunk (default: 16000)

### 2. Configure SLURM

Edit `vllm_inference.sbatch`:
- **Line 30**: Update environment source path for your cluster
- GPU configuration: Default is 4x A40

Edit `submit_vllm_inference_jobs.sh`:
- `FILES`: List of dataset names to process
- `MODELS`: List of HuggingFace model IDs
- `CHECKLIST_ITEMS`: Specific items or empty for all 26
- `DEFAULT_ENABLE_THINKING`: Enable thinking mode (true/false)

### 3. Submit Jobs

```bash
./submit_vllm_inference_jobs.sh
```

### 4. Monitor Progress

```bash
# View SLURM job status
squeue -u $USER

# Watch inference logs
tail -f vllm_inference_logs/*.out
```

---

## Input/Output Formats

### Input (from notebook)

```json
{
  "keys": [
    ["46507", "Complaint for Declaratory Relief"],
    ["46507", "Order Granting TRO"],
    ["46210", "Motion to Dismiss"]
  ],
  "chunks": [
    ["chunk1 text...", "chunk2 text..."],
    ["chunk1 text..."],
    ["chunk1 text...", "chunk2 text...", "chunk3 text..."]
  ],
  "chunks_tokens": [
    [16000, 8500],
    [12000],
    [16000, 16000, 5000]
  ]
}
```

### Output

```json
{
  "meta_data": {
    "file_name": "20_human_eval_cases",
    "inference_model": "Qwen/Qwen3-14B",
    "enable_thinking": true
  },
  "token_stats": {
    "total_input_tokens": 500000,
    "total_output_tokens": 250000,
    "num_prompts": 5200
  },
  "results": {
    "46507": {
      "Filing Date": {
        "reasoning": "Found filing date in complaint header...",
        "extracted": [
          {
            "value": "April 28, 2025",
            "evidence": "Filed 04/28/25",
            "source_document": "Complaint",
            "source_chunk": 1
          }
        ]
      },
      "Cause of Action": { ... }
    }
  }
}
```

---

## Supported Models

| Model | Thinking Mode | Notes |
|-------|---------------|-------|
| `Qwen/Qwen3-8B` | `<think>` tags | Smaller, faster |
| `Qwen/Qwen3-14B` | `<think>` tags | Good balance |
| `Qwen/Qwen3-32B` | `<think>` tags | Higher quality |
| `Qwen/Qwen3-30B-A3B-Thinking-2507` | `<think>` tags | Auto-enabled |
| `unsloth/gpt-oss-20b-BF16` | Channel-based | Different format |
| `google/gemma-3-12b-it` | Not supported | No thinking |
| `google/gemma-3-27b-it` | Not supported | No thinking |

Thinking mode is auto-detected for `*-Thinking-2507` models and disabled for `*-Instruct-2507` models.

---

## Configuration Options

### Command Line Arguments

| Argument | Description | Default |
|----------|-------------|---------|
| `--file_name` | Dataset name (without .json) | Required |
| `--model_name` | HuggingFace model ID | `Qwen/Qwen3-14B` |
| `--enable_thinking` | Enable thinking mode | `false` |
| `--checklist_item` | Extract specific item only | All items |

### Example Direct Run

```bash
python vllm_inference.py \
    --file_name 20_human_eval_cases \
    --model_name Qwen/Qwen3-14B \
    --enable_thinking
```

---

## Checkpointing

The pipeline saves checkpoints after each chunk iteration to `checkpoints/`. If interrupted:

1. Rerun the same command
2. The script automatically detects the last completed chunk
3. Processing resumes from where it stopped

Checkpoint files: `checkpoints/{model}/{dataset}_chunk_{n}_thinking_{bool}.json`

---

## Troubleshooting

### GPU Out of Memory

- Reduce `gpu_memory_utilization` in `vllm_inference.py` (default: 0.9)
- Use a smaller model variant
- Reduce tensor parallel size in sbatch file

### Missing Results

- Check `vllm_inference_logs/` for error messages
- Verify checkpoint files exist in `checkpoints/`
- Rerun the same command to resume from checkpoint

### Thinking Mode Not Working

- Verify model supports thinking (Qwen3 or GPT-OSS)
- Check `--enable_thinking` flag is set
- For GPT-OSS, ensure `skip_special_tokens=False`

---

## Related Documentation

- [Prompt Template](../../../../prompts/extract_checklist_item_from_docs/chunk_by_chunk_template.txt) - The extraction prompt template
- [Checklist Items](../../../../prompts/extract_checklist_item_from_docs/item_specific_info.json) - 26 item definitions
- [Main Project README](../../../../README.md) - Project overview
- [Data README](../../../../data/README.md) - Data structure documentation
