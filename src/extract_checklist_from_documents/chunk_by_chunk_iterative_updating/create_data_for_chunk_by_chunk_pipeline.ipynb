{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print current environment python\n",
    "import sys\n",
    "print(sys.executable)\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/coc/pskynet6/douy/legal-envs/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing tokenizers...\n",
      "Tokenizers initialized successfully\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import tiktoken\n",
    "import math\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "# Initialize tokenizers\n",
    "print(\"Initializing tokenizers...\")\n",
    "\n",
    "# GPT-4o tokenizer\n",
    "gpt4_enc = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "\n",
    "# HuggingFace tokenizers\n",
    "gemma_tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-3-4b-it\")\n",
    "qwen_tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-8B\")\n",
    "\n",
    "print(\"Tokenizers initialized successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_single_document(doc_text, max_chunk_tokens=16000):\n",
    "    \"\"\"\n",
    "    Chunk a single document into smaller pieces based on token count.\n",
    "    Uses multiple tokenizers and takes the maximum token count.\n",
    "    \n",
    "    Args:\n",
    "        doc_text: Document string to chunk\n",
    "        max_chunk_tokens: Maximum tokens per chunk (default: 16000)\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (chunks_list, tokens_list) where:\n",
    "        - chunks_list: List of document chunks\n",
    "        - tokens_list: List of token counts for each chunk\n",
    "    \"\"\"\n",
    "    # Get token counts from each tokenizer\n",
    "    gpt4_tokens = gpt4_enc.encode(doc_text)\n",
    "    gemma_tokens = gemma_tokenizer.encode(doc_text, add_special_tokens=False)\n",
    "    qwen_tokens = qwen_tokenizer.encode(doc_text, add_special_tokens=False)\n",
    "    \n",
    "    # Take the maximum token count\n",
    "    gpt4_count = len(gpt4_tokens)\n",
    "    gemma_count = len(gemma_tokens)\n",
    "    qwen_count = len(qwen_tokens)\n",
    "    max_token_count = max(gpt4_count, gemma_count, qwen_count)\n",
    "    \n",
    "    # Determine which tokenizer gave the maximum\n",
    "    if max_token_count == gpt4_count:\n",
    "        tokens = gpt4_tokens\n",
    "        tokenizer_type = 'gpt4'\n",
    "    elif max_token_count == gemma_count:\n",
    "        tokens = gemma_tokens\n",
    "        tokenizer_type = 'gemma'\n",
    "    else:\n",
    "        tokens = qwen_tokens\n",
    "        tokenizer_type = 'qwen'\n",
    "    \n",
    "    # If document is within limit, return as single chunk\n",
    "    if max_token_count <= max_chunk_tokens:\n",
    "        return [doc_text], [max_token_count]\n",
    "    \n",
    "    # Otherwise, chunk the document\n",
    "    chunks = []\n",
    "    chunk_tokens = []\n",
    "    \n",
    "    # Calculate number of chunks needed\n",
    "    num_chunks = math.ceil(max_token_count / max_chunk_tokens)\n",
    "    \n",
    "    for i in range(num_chunks):\n",
    "        start_idx = i * max_chunk_tokens\n",
    "        end_idx = min((i + 1) * max_chunk_tokens, len(tokens))\n",
    "        \n",
    "        chunk_token_ids = tokens[start_idx:end_idx]\n",
    "        \n",
    "        # Decode chunk based on tokenizer type\n",
    "        if tokenizer_type == 'gpt4':\n",
    "            chunk_text = gpt4_enc.decode(chunk_token_ids)\n",
    "        elif tokenizer_type == 'gemma':\n",
    "            chunk_text = gemma_tokenizer.decode(chunk_token_ids, skip_special_tokens=True)\n",
    "        else:  # qwen\n",
    "            chunk_text = qwen_tokenizer.decode(chunk_token_ids, skip_special_tokens=True)\n",
    "        \n",
    "        # Ensure we don't cut in middle of a word (for last chunk of each split)\n",
    "        if i < num_chunks - 1 and chunk_text and not chunk_text[-1].isspace():\n",
    "            # Find the last complete word boundary\n",
    "            last_boundary = max(\n",
    "                chunk_text.rfind(' '),\n",
    "                chunk_text.rfind('\\n'),\n",
    "                chunk_text.rfind('\\t'),\n",
    "                chunk_text.rfind('.'),\n",
    "                chunk_text.rfind(','),\n",
    "                chunk_text.rfind('!'),\n",
    "                chunk_text.rfind('?'),\n",
    "                chunk_text.rfind(';'),\n",
    "                chunk_text.rfind(':')\n",
    "            )\n",
    "            \n",
    "            if last_boundary > 0:\n",
    "                # Adjust the chunk text and recalculate tokens\n",
    "                chunk_text = chunk_text[:last_boundary + 1]\n",
    "                \n",
    "                # Recalculate actual token count for this chunk\n",
    "                actual_gpt4 = len(gpt4_enc.encode(chunk_text))\n",
    "                actual_gemma = len(gemma_tokenizer.encode(chunk_text, add_special_tokens=False))\n",
    "                actual_qwen = len(qwen_tokenizer.encode(chunk_text, add_special_tokens=False))\n",
    "                actual_tokens = max(actual_gpt4, actual_gemma, actual_qwen)\n",
    "            else:\n",
    "                actual_tokens = len(chunk_token_ids)\n",
    "        else:\n",
    "            # For the last chunk, calculate actual tokens\n",
    "            actual_gpt4 = len(gpt4_enc.encode(chunk_text))\n",
    "            actual_gemma = len(gemma_tokenizer.encode(chunk_text, add_special_tokens=False))\n",
    "            actual_qwen = len(qwen_tokenizer.encode(chunk_text, add_special_tokens=False))\n",
    "            actual_tokens = max(actual_gpt4, actual_gemma, actual_qwen)\n",
    "        \n",
    "        chunks.append(chunk_text)\n",
    "        chunk_tokens.append(actual_tokens)\n",
    "    \n",
    "    return chunks, chunk_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "input_file_name = \"20_human_eval_cases\"  # Update this as needed (without .json extension)\n",
    "order_by_date = True\n",
    "max_chunk_tokens = 16000  # Maximum tokens per chunk\n",
    "\n",
    "# Input path - from data/full_case_data folder\n",
    "input_path = f\"../../../../data/full_case_data/{input_file_name}.json\"\n",
    "\n",
    "# Saving path - to the local data folder\n",
    "saving_path = f\"./data/{input_file_name}.json\"\n",
    "\n",
    "print(f\"Input file: {input_path}\")\n",
    "print(f\"Order by date: {order_by_date}\")\n",
    "print(f\"Max tokens per chunk: {max_chunk_tokens}\")\n",
    "print(f\"Output path: {saving_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "with open(input_path, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(data)} cases from {input_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing cases:   0%|                                                                                                                                                                                                                                 | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Case 46507, Document 'Complaint for Declaratory and Injunctive Relief': 6 chunks, 82441 total tokens\n",
      "  Case 46507, Document 'Order Granting Temporary Restraining Order and Compelling Certain Discovery Production': 4 chunks, 63047 total tokens\n",
      "  Case 46507, Document 'Order Granting Temporary Restraining Order and Compelling Certain Discovery Production': 3 chunks, 35856 total tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (298491 > 131072). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Case 46507, Document 'Addendum to Emergency Motion for Stay Pending Appeal/Petition for Writ of Mandamus': 20 chunks, 314770 total tokens\n",
      "  Case 46507, Document 'Plaintiffs' Memorandum in Support of Motion for Preliminary Injunction': 3 chunks, 42629 total tokens\n",
      "  Case 46507, Document 'Defendants' Opposition to Plaintiffs' Motion for Preliminary Injunction; Memorandum of Points and Authorities': 2 chunks, 25500 total tokens\n",
      "  Case 46507, Document 'Order Granting Preliminary Injunction': 3 chunks, 43390 total tokens\n",
      "  Case 46507, Document 'Order': 2 chunks, 20940 total tokens\n",
      "  Case 46507, Document 'Application to Stay the Order Issued by the United States District Court for the Northern District of California and Request for an Immediate Administrative Stay': 6 chunks, 91362 total tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing cases:   5%|██████████▊                                                                                                                                                                                                              | 1/20 [00:08<02:35,  8.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Case 46507, Document 'Docket': 4 chunks, 51493 total tokens\n",
      "  Case 46329, Document 'Complaint for Declaratory and Injunctive Relief': 3 chunks, 39241 total tokens\n",
      "  Case 46329, Document 'Memorandum of Law in Support of Plaintiffs' Motion for Preliminary Injunction': 3 chunks, 37418 total tokens\n",
      "  Case 46329, Document 'Memorandum of Points and Authorities in Support of Defendants' Motion to Dismiss Plaintiffs' Unreasonable Delay Claims': 2 chunks, 17212 total tokens\n",
      "  Case 46329, Document 'Findings of Fact, Rulings of Law, and Order for Partial Separate and Final Judgment': 3 chunks, 32416 total tokens\n",
      "  Case 46329, Document '': 2 chunks, 17568 total tokens\n",
      "  Case 46329, Document '': 2 chunks, 20947 total tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing cases:  10%|█████████████████████▋                                                                                                                                                                                                   | 2/20 [00:10<01:24,  4.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Case 46329, Document 'Docket': 2 chunks, 22972 total tokens\n",
      "  Case 46758, Document 'Order': 2 chunks, 28187 total tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing cases:  15%|████████████████████████████████▌                                                                                                                                                                                        | 3/20 [00:11<00:49,  2.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Case 46758, Document 'Docket': 3 chunks, 36625 total tokens\n",
      "  Case 46666, Document 'Class Action Complaint for Declaratory and Injunctive Relief': 5 chunks, 79604 total tokens\n",
      "  Case 46666, Document 'Plaintiffs' Motion for Class Certification and Memorandum of Points and Authorities in Support': 2 chunks, 18780 total tokens\n",
      "  Case 46666, Document 'Order Granting Motion for Preliminary Injunction and Provisional Class Certification': 3 chunks, 40500 total tokens\n",
      "  Case 46666, Document 'Motion for Partial Stay Pending Appeal Relief Requested by August 4, 2025': 4 chunks, 54468 total tokens\n",
      "  Case 46666, Document 'Amended Class Action Complaint for Declaratory and Injunctive Relief': 7 chunks, 99430 total tokens\n",
      "  Case 46666, Document 'Motion for Preliminary Injunction and Provisional Class Certification as to Additional Agency Defendants': 2 chunks, 26672 total tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing cases:  20%|███████████████████████████████████████████▍                                                                                                                                                                             | 4/20 [00:15<00:53,  3.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Case 46666, Document 'Docket': 2 chunks, 21991 total tokens\n",
      "  Case 46746, Document 'Omnibus Order': 4 chunks, 52345 total tokens\n",
      "  Case 46746, Document '': 9 chunks, 141571 total tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing cases:  25%|██████████████████████████████████████████████████████▎                                                                                                                                                                  | 5/20 [00:17<00:46,  3.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Case 46746, Document 'Motion to Stay Preliminary Injunction Pending Appeal and for Administrative Stay': 2 chunks, 23515 total tokens\n",
      "  Case 46746, Document 'Docket': 2 chunks, 16786 total tokens\n",
      "  Case 46678, Document 'Plaintiffs' Ex Parte Motion for a Temporary Restraining Order': 2 chunks, 20154 total tokens\n",
      "  Case 46678, Document 'Defendants' Corrected Opposition to Plaintiffs' Motion for a Temporary Restraining Order': 2 chunks, 23082 total tokens\n",
      "  Case 46678, Document 'Order Granting Plaintiffs' Application for Temporary Restraining Order': 2 chunks, 28542 total tokens\n",
      "  Case 46678, Document 'Plaintiffs' Motion for Preliminary Injunction': 2 chunks, 29552 total tokens\n",
      "  Case 46678, Document 'Order': 2 chunks, 18560 total tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing cases:  30%|█████████████████████████████████████████████████████████████████                                                                                                                                                        | 6/20 [00:19<00:36,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Case 46678, Document 'Docket': 2 chunks, 25482 total tokens\n",
      "  Case 46390, Document 'Order': 2 chunks, 20264 total tokens\n",
      "  Case 46390, Document 'Emergency Motion for a Stay Pending Appeal': 4 chunks, 53799 total tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing cases:  35%|███████████████████████████████████████████████████████████████████████████▉                                                                                                                                             | 7/20 [00:20<00:27,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Case 46348, Document 'Memorandum of Points and Authorities Supporting Plaintiff American Foreign Service Association's Motion for Preliminary Injunction': 3 chunks, 35509 total tokens\n",
      "  Case 46348, Document 'Opinion': 2 chunks, 22021 total tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing cases:  40%|██████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                  | 8/20 [00:21<00:21,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Case 46348, Document 'Emergency Motion for an Immediate Administrative Stay and Stay Pending Appeal': 3 chunks, 33534 total tokens\n",
      "  Case 46340, Document 'Complaint for Declaratory and Injunctive Relief': 2 chunks, 29645 total tokens\n",
      "  Case 46340, Document 'Order Re Preliminary Injunction': 2 chunks, 22843 total tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing cases:  45%|█████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                                       | 9/20 [00:22<00:16,  1.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Case 46340, Document 'Docket': 2 chunks, 19109 total tokens\n",
      "  Case 46482, Document 'Complaint': 2 chunks, 17346 total tokens\n",
      "  Case 46482, Document 'Plaintiffs’ Memorandum of Law in Support of Their Motion for a Preliminary Injunction': 2 chunks, 20919 total tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing cases:  50%|████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                            | 10/20 [00:23<00:13,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Case 46482, Document 'Memorandum in Opposition to Plaintiff's Motion for a Preliminary Injunction': 3 chunks, 32962 total tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing cases:  55%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                 | 11/20 [00:24<00:09,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Case 46755, Document 'Memorandum Opinion': 4 chunks, 48721 total tokens\n",
      "  Case 46341, Document 'Complaint for Declaratory and Injunctive Relief': 3 chunks, 45576 total tokens\n",
      "  Case 46341, Document 'Amended Complaint': 4 chunks, 51113 total tokens\n",
      "  Case 46341, Document 'Memorandum of Law in Support of Plaintiffs’ Motion for a Preliminary Injunction': 3 chunks, 35384 total tokens\n",
      "  Case 46341, Document 'Time Sensistive Motion for Stay Pending Appeal and Immediate Administrative Stay': 15 chunks, 240098 total tokens\n",
      "  Case 46341, Document '': 2 chunks, 17568 total tokens\n",
      "  Case 46341, Document '': 2 chunks, 20947 total tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing cases:  60%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                      | 12/20 [00:29<00:18,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Case 46341, Document 'Docket': 2 chunks, 28828 total tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing cases:  65%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                           | 13/20 [00:29<00:12,  1.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Case 46342, Document 'First Amended Complaint for Declaratory and Injunctive Relief': 2 chunks, 16763 total tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing cases:  70%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                | 14/20 [00:29<00:07,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Case 46651, Document 'Docket': 2 chunks, 17264 total tokens\n",
      "  Case 46351, Document 'Complaint for Declaratory and Injunctive Relief': 2 chunks, 22505 total tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing cases:  75%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                      | 15/20 [00:30<00:05,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Case 46351, Document 'First Amended Complaint for Declaratory and Injunctive Relief': 2 chunks, 25981 total tokens\n",
      "  Case 46620, Document 'Memorandum Opinion': 2 chunks, 19704 total tokens\n",
      "  Case 46620, Document 'Time-Sensitive Motion for Stay Pending Appeal and an Administrative Stay': 4 chunks, 48129 total tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing cases:  80%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                           | 16/20 [00:31<00:04,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Case 46620, Document 'Opposition to Application to Stay the Judgment of the United States District Court for the District of Maryland': 2 chunks, 17706 total tokens\n",
      "  Case 46625, Document 'Class Action Complaint for Declaratory and Injunctive Relief': 2 chunks, 17654 total tokens\n",
      "  Case 46625, Document 'Defendants' Motion to Dismiss and Memorandum in Opposition to Plaintiffs' Motion for Preliminary Injunction': 3 chunks, 36302 total tokens\n",
      "  Case 46625, Document 'Memorandum Opinion': 2 chunks, 22753 total tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing cases:  85%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                | 17/20 [00:34<00:04,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Case 46625, Document 'Plaintiffs-Appellants' Emergency Motion for an Injunction Pending Appeal': 8 chunks, 119965 total tokens\n",
      "  Case 46499, Document 'Notice of Motion and Motion for Temporary Restraining Order; Memorandum of Law in Support of Plaintiffs' Motion for Nationwide Temporary Restraining Order': 2 chunks, 23256 total tokens\n",
      "  Case 46499, Document 'Order Granting Motions for Preliminary Injunctions and Setting Case Management Conferences': 2 chunks, 17431 total tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing cases:  90%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                     | 18/20 [00:35<00:02,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Case 46499, Document 'Motion for Amended Preliminary Injunction': 2 chunks, 30984 total tokens\n",
      "  Case 46602, Document 'Complaint': 2 chunks, 20233 total tokens\n",
      "  Case 46602, Document 'Amended Complaint for Injunctive and Declaratory Relief': 2 chunks, 23945 total tokens\n",
      "  Case 46602, Document 'Memorandum in Support of Plaintiffs' Motion for Preliminary Injunction': 2 chunks, 24334 total tokens\n",
      "  Case 46602, Document 'Brief of Amicus Curiae States Of Oregon, Maryland, Washington, Arizona, Colorado, Connecticut; Delaware, Hawai‘i, Maine, Michigan, Nevada, New Mexico, Rhode Island, Wisconsin, Vermont, And The District Of Columbia in Support of Plaintiffs’ Motion for Preliminary Injunction': 2 chunks, 23380 total tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing cases:  95%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏          | 19/20 [00:36<00:01,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Case 46602, Document 'Opinion and Order': 4 chunks, 49091 total tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing cases: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:37<00:00,  1.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Case 46805, Document 'Complaint for Declaratory and Injunctive Relief': 3 chunks, 44731 total tokens\n",
      "\n",
      "Processed 176 documents total\n",
      "Documents chunked: 75\n",
      "Total chunks created: 344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Process documents and create chunks\n",
    "random.seed(42)\n",
    "\n",
    "keys = []  # List of (case_id, document_name) tuples\n",
    "chunks = []  # List of list of chunks\n",
    "chunks_tokens = []  # List of list of token counts\n",
    "\n",
    "# Process each case\n",
    "for item in tqdm(data, desc=\"Processing cases\"):\n",
    "    case_id = item[\"case_id\"]\n",
    "    case_documents_title = item[\"case_documents_title\"]\n",
    "    case_documents_date = item[\"case_documents_date\"]\n",
    "    case_documents_text = item[\"case_documents_text\"]\n",
    "    \n",
    "    if order_by_date:\n",
    "        # Zip and sort by date\n",
    "        zipped_data = list(zip(case_documents_date, case_documents_title, case_documents_text))\n",
    "        sorted_data = sorted(zipped_data, key=lambda x: (x[0] is None, x[0]))\n",
    "        \n",
    "        # Unzip back to lists\n",
    "        case_documents_date, case_documents_title, case_documents_text = zip(*sorted_data)\n",
    "        case_documents_date = list(case_documents_date)\n",
    "        case_documents_title = list(case_documents_title)\n",
    "        case_documents_text = list(case_documents_text)\n",
    "        \n",
    "        # Move \"Docket\" entries to the end\n",
    "        docket_indices = [i for i, title in enumerate(case_documents_title) if \"Docket\" in title]\n",
    "        \n",
    "        docket_items = []\n",
    "        for idx in reversed(docket_indices):\n",
    "            docket_items.append((\n",
    "                case_documents_date.pop(idx),\n",
    "                case_documents_title.pop(idx),\n",
    "                case_documents_text.pop(idx)\n",
    "            ))\n",
    "        \n",
    "        for date, title, text in reversed(docket_items):\n",
    "            case_documents_date.append(date)\n",
    "            case_documents_title.append(title)\n",
    "            case_documents_text.append(text)\n",
    "    else:\n",
    "        # Random order with seed 42\n",
    "        indices = list(range(len(case_documents_title)))\n",
    "        random.shuffle(indices)\n",
    "        \n",
    "        case_documents_title = [case_documents_title[i] for i in indices]\n",
    "        case_documents_text = [case_documents_text[i] for i in indices]\n",
    "        case_documents_date = [case_documents_date[i] for i in indices]\n",
    "    \n",
    "    # Process each document in the case\n",
    "    for doc_title, doc_text in zip(case_documents_title, case_documents_text):\n",
    "        # Chunk the document\n",
    "        doc_chunks, doc_tokens = chunk_single_document(doc_text, max_chunk_tokens)\n",
    "        \n",
    "        # Add to lists\n",
    "        keys.append((case_id, doc_title))\n",
    "        chunks.append(doc_chunks)\n",
    "        chunks_tokens.append(doc_tokens)\n",
    "        \n",
    "        # Print info for documents that were chunked\n",
    "        if len(doc_chunks) > 1:\n",
    "            total_tokens = sum(doc_tokens)\n",
    "            print(f\"  Case {case_id}, Document '{doc_title}': {len(doc_chunks)} chunks, {total_tokens} total tokens\")\n",
    "\n",
    "print(f\"\\nProcessed {len(keys)} documents total\")\n",
    "print(f\"Documents chunked: {sum(1 for c in chunks if len(c) > 1)}\")\n",
    "print(f\"Total chunks created: {sum(len(c) for c in chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of keys: 176\n",
      "Number of chunk lists: 176\n",
      "Number of token lists: 176\n",
      "\n",
      "Sample of first 3 entries:\n",
      "  Key: ('46507', 'Complaint for Declaratory and Injunctive Relief')\n",
      "  Number of chunks: 6\n",
      "  Token counts: [16000, 16000, 16000, 16000, 15997, 2444]\n",
      "  First chunk preview (first 100 chars): Case 3:25-cv-03698     Document 1     Filed 04/28/25     Page 1 of 115\n",
      " \n",
      " \n",
      "1  Stacey M. Leyton (SBN ...\n",
      "\n",
      "  Key: ('46507', 'Order Granting Temporary Restraining Order and Compelling Certain Discovery Production')\n",
      "  Number of chunks: 4\n",
      "  Token counts: [16000, 16000, 16000, 15047]\n",
      "  First chunk preview (first 100 chars): No. 24A          \n",
      " \n",
      " \n",
      "In the Supreme Court of the United States \n",
      " \n",
      " \n",
      "─────────── \n",
      " \n",
      " \n",
      "DONALD J. TRUM...\n",
      "\n",
      "  Key: ('46507', 'Order Granting Temporary Restraining Order and Compelling Certain Discovery Production')\n",
      "  Number of chunks: 3\n",
      "  Token counts: [16000, 16000, 3856]\n",
      "  First chunk preview (first 100 chars): Case 3:25-cv-03698-SI     Document 85     Filed 05/09/25     Page 1 of 42\n",
      " \n",
      "1   \n",
      "2   \n",
      "3   \n",
      "4  UNITED...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Verify the data structure\n",
    "print(f\"Number of keys: {len(keys)}\")\n",
    "print(f\"Number of chunk lists: {len(chunks)}\")\n",
    "print(f\"Number of token lists: {len(chunks_tokens)}\")\n",
    "\n",
    "# Show sample of the data structure\n",
    "print(\"\\nSample of first 3 entries:\")\n",
    "for i in range(min(3, len(keys))):\n",
    "    key = keys[i]\n",
    "    chunk_list = chunks[i]\n",
    "    token_list = chunks_tokens[i]\n",
    "    print(f\"  Key: {key}\")\n",
    "    print(f\"  Number of chunks: {len(chunk_list)}\")\n",
    "    print(f\"  Token counts: {token_list}\")\n",
    "    if len(chunk_list) > 0:\n",
    "        print(f\"  First chunk preview (first 100 chars): {chunk_list[0][:100]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk data to ../../../../batch_api/chunk_by_chunk_iterative_updating/data/20_human_eval_cases_2.json\n",
      "File contains:\n",
      "  - 176 document entries\n",
      "  - 344 total chunks\n",
      "  - Average chunks per document: 1.95\n"
     ]
    }
   ],
   "source": [
    "# Save results for batch processing\n",
    "saving_folder = os.path.dirname(saving_path)\n",
    "\n",
    "if not os.path.exists(saving_folder):\n",
    "    os.makedirs(saving_folder)\n",
    "\n",
    "results_dict = {\n",
    "    \"keys\": keys,\n",
    "    \"chunks\": chunks,\n",
    "    \"chunks_tokens\": chunks_tokens\n",
    "}\n",
    "\n",
    "with open(saving_path, \"w\") as f:\n",
    "    json.dump(results_dict, f, indent=4)\n",
    "\n",
    "print(f\"Saved chunk data to {saving_path}\")\n",
    "print(f\"File contains:\")\n",
    "print(f\"  - {len(keys)} document entries\")\n",
    "print(f\"  - {sum(len(c) for c in chunks)} total chunks\")\n",
    "print(f\"  - Average chunks per document: {sum(len(c) for c in chunks) / len(chunks):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "CHUNKING STATISTICS SUMMARY\n",
      "==================================================\n",
      "Total documents processed: 176\n",
      "Documents kept as single chunk: 101 (57.4%)\n",
      "Documents split into multiple chunks: 75 (42.6%)\n",
      "Total chunks created: 344\n",
      "Average chunks per document: 1.95\n",
      "Average chunks for chunked documents: 3.24\n",
      "\n",
      "Token Statistics:\n",
      "  Min tokens per chunk: 81\n",
      "  Max tokens per chunk: 16,184\n",
      "  Avg tokens per chunk: 10,989\n",
      "  Total tokens: 3,780,068\n"
     ]
    }
   ],
   "source": [
    "# Statistics summary\n",
    "print(\"=\" * 50)\n",
    "print(\"CHUNKING STATISTICS SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "total_docs = len(keys)\n",
    "chunked_docs = sum(1 for c in chunks if len(c) > 1)\n",
    "single_chunk_docs = total_docs - chunked_docs\n",
    "total_chunks = sum(len(c) for c in chunks)\n",
    "\n",
    "print(f\"Total documents processed: {total_docs}\")\n",
    "print(f\"Documents kept as single chunk: {single_chunk_docs} ({single_chunk_docs/total_docs*100:.1f}%)\")\n",
    "print(f\"Documents split into multiple chunks: {chunked_docs} ({chunked_docs/total_docs*100:.1f}%)\")\n",
    "print(f\"Total chunks created: {total_chunks}\")\n",
    "print(f\"Average chunks per document: {total_chunks/total_docs:.2f}\")\n",
    "\n",
    "if chunked_docs > 0:\n",
    "    avg_chunks_for_chunked = sum(len(c) for c in chunks if len(c) > 1) / chunked_docs\n",
    "    print(f\"Average chunks for chunked documents: {avg_chunks_for_chunked:.2f}\")\n",
    "\n",
    "# Token statistics\n",
    "all_tokens = [t for token_list in chunks_tokens for t in token_list]\n",
    "if all_tokens:\n",
    "    print(f\"\\nToken Statistics:\")\n",
    "    print(f\"  Min tokens per chunk: {min(all_tokens):,}\")\n",
    "    print(f\"  Max tokens per chunk: {max(all_tokens):,}\")\n",
    "    print(f\"  Avg tokens per chunk: {sum(all_tokens)/len(all_tokens):,.0f}\")\n",
    "    print(f\"  Total tokens: {sum(all_tokens):,}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
