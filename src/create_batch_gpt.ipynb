{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Batch Job for OpenAI/Azure API\n",
    "\n",
    "This notebook creates a batch job for processing multiple prompts through OpenAI or Azure OpenAI API.\n",
    "\n",
    "## Features\n",
    "- Supports GPT-4.1, o3, and GPT-5 models\n",
    "- Optional Azure OpenAI deployment\n",
    "- Reasoning effort configuration for o3/GPT-5 (low, medium, high)\n",
    "- Batch metadata saved for result retrieval\n",
    "\n",
    "## Prerequisites\n",
    "- Set the `OPENAI_API_KEY` environment variable (or `AZURE_OPENAI_API_KEY` for Azure)\n",
    "- Prepare input JSON file with `keys` and `contexts` arrays\n",
    "\n",
    "## Input Format\n",
    "```json\n",
    "{\n",
    "  \"keys\": [\"case_001\", \"case_002\", ...],\n",
    "  \"contexts\": [\n",
    "    [{\"role\": \"user\", \"content\": \"...\"}],\n",
    "    [{\"role\": \"system\", \"content\": \"...\"}, {\"role\": \"user\", \"content\": \"...\"}],\n",
    "    ...\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "## Reasoning Effort (o3/GPT-5 only)\n",
    "- `low`: Faster, less thorough reasoning\n",
    "- `medium`: Balanced (default for o3/GPT-5)\n",
    "- `high`: Most thorough reasoning, slower\n",
    "\n",
    "## Output\n",
    "- JSONL file uploaded to OpenAI\n",
    "- Batch metadata saved to `logs/{INPUT_DIR}/{MODEL_NAME}/{INPUT_FILE}_thinking_{effort}.json`\n",
    "- Use `retrieve_batch_results_gpt.ipynb` to get results after batch completes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION - Edit these variables before running\n",
    "# ============================================================================\n",
    "from pathlib import Path\n",
    "\n",
    "# Base directory for data files\n",
    "BASE_DIR = Path(\".\")  # Change to your data directory\n",
    "\n",
    "# Input file settings\n",
    "INPUT_DIR = \"data\"  # Directory containing input JSON file\n",
    "INPUT_FILE = \"your_input_file\"  # Name without .json extension\n",
    "\n",
    "# Model settings\n",
    "# Supported models: gpt-4.1-2025-04-14, o3-2025-04-16, gpt-5-2025-08-07\n",
    "MODEL_NAME = \"gpt-4.1-2025-04-14\"\n",
    "\n",
    "# Reasoning effort (required for o3 and gpt-5, ignored for other models)\n",
    "# Options: \"low\", \"medium\", \"high\", or None\n",
    "REASONING_EFFORT = \"medium\"\n",
    "\n",
    "# Azure OpenAI settings (set USE_AZURE=True to use Azure)\n",
    "USE_AZURE = False\n",
    "AZURE_ENDPOINT = \"https://your-resource.openai.azure.com/\"  # Your Azure endpoint\n",
    "AZURE_DEPLOYMENT_MAP = {  # Map model names to Azure deployment names\n",
    "    \"o3-2025-04-16\": \"o3-batch\",\n",
    "    \"gpt-4.1-2025-04-14\": \"gpt-4.1-batch\",\n",
    "}\n",
    "\n",
    "# Output directories\n",
    "LOGS_DIR = BASE_DIR / \"logs\"\n",
    "JSONL_DIR = BASE_DIR / \"openai_jsonl\"\n",
    "\n",
    "# ============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from openai import OpenAI, AzureOpenAI\n",
    "\n",
    "# Determine if reasoning effort applies to this model\n",
    "is_reasoning_model = \"o3\" in MODEL_NAME or \"gpt-5\" in MODEL_NAME\n",
    "thinking_effort = REASONING_EFFORT if is_reasoning_model else None\n",
    "\n",
    "# Default to medium for reasoning models if not specified\n",
    "if is_reasoning_model and not thinking_effort:\n",
    "    thinking_effort = \"medium\"\n",
    "\n",
    "# Get Azure deployment name if using Azure\n",
    "deployment_name = None\n",
    "if USE_AZURE:\n",
    "    if MODEL_NAME not in AZURE_DEPLOYMENT_MAP:\n",
    "        raise ValueError(f\"Model {MODEL_NAME} not found in AZURE_DEPLOYMENT_MAP\")\n",
    "    deployment_name = AZURE_DEPLOYMENT_MAP[MODEL_NAME]\n",
    "\n",
    "# Initialize client\n",
    "if USE_AZURE:\n",
    "    client = AzureOpenAI(\n",
    "        api_version=\"2025-03-01-preview\",\n",
    "        azure_endpoint=AZURE_ENDPOINT,\n",
    "        api_key=os.environ.get(\"AZURE_OPENAI_API_KEY\"),\n",
    "    )\n",
    "else:\n",
    "    client = OpenAI(\n",
    "        api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    "    )\n",
    "\n",
    "# Build paths\n",
    "input_path = BASE_DIR / INPUT_DIR / f\"{INPUT_FILE}.json\"\n",
    "thinking_suffix = f\"_thinking_{thinking_effort}\" if thinking_effort else \"\"\n",
    "jsonl_path = JSONL_DIR / INPUT_DIR / MODEL_NAME / f\"{INPUT_FILE}{thinking_suffix}.jsonl\"\n",
    "jsonl_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load input data\n",
    "with open(input_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(f\"Input file: {input_path}\")\n",
    "print(f\"Loaded {len(data['keys'])} cases\")\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Using Azure: {USE_AZURE}\")\n",
    "if thinking_effort:\n",
    "    print(f\"Reasoning effort: {thinking_effort}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build JSONL batch requests\n",
    "requests = []\n",
    "key_dict = {}\n",
    "\n",
    "for i, (key, context) in enumerate(list(zip(data[\"keys\"], data[\"contexts\"]))):\n",
    "    batch_request = {\n",
    "        \"custom_id\": str(i),\n",
    "        \"method\": \"POST\",\n",
    "        \"url\": \"/chat/completions\" if USE_AZURE else \"/v1/chat/completions\",\n",
    "        \"body\": {\n",
    "            \"model\": deployment_name if USE_AZURE else MODEL_NAME,\n",
    "            \"messages\": context,\n",
    "            \"temperature\": 1.0 if is_reasoning_model else 0.7,\n",
    "            \"top_p\": 1.0,\n",
    "            \"max_completion_tokens\": 50000 if is_reasoning_model else 32000,\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Add reasoning_effort for o3/gpt-5 models\n",
    "    if thinking_effort:\n",
    "        batch_request[\"body\"][\"reasoning_effort\"] = thinking_effort\n",
    "\n",
    "    key_dict[str(i)] = key\n",
    "    requests.append(batch_request)\n",
    "\n",
    "# Write to JSONL file\n",
    "with open(jsonl_path, \"w\") as f:\n",
    "    for r in requests:\n",
    "        json.dump(r, f)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(f\"Created {len(requests)} batch requests\")\n",
    "print(f\"Saved to: {jsonl_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload JSONL file and create batch job\n",
    "batch_input_file = client.files.create(\n",
    "    file=open(jsonl_path, \"rb\"),\n",
    "    purpose=\"batch\"\n",
    ")\n",
    "\n",
    "print(f\"Uploaded file:\")\n",
    "print(f\"  ID: {batch_input_file.id}\")\n",
    "print(f\"  Filename: {batch_input_file.filename}\")\n",
    "print(f\"  Status: {batch_input_file.status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create batch job\n",
    "batch_result = client.batches.create(\n",
    "    input_file_id=batch_input_file.id,\n",
    "    endpoint=\"/chat/completions\" if USE_AZURE else \"/v1/chat/completions\",\n",
    "    completion_window=\"24h\",\n",
    "    metadata={\n",
    "        \"description\": \"batch processing\",\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"Created batch job:\")\n",
    "print(f\"  ID: {batch_result.id}\")\n",
    "print(f\"  Status: {batch_result.status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check batch status (re-run this cell to check progress)\n",
    "batch_result = client.batches.retrieve(batch_result.id)\n",
    "\n",
    "print(f\"Batch ID: {batch_result.id}\")\n",
    "print(f\"Status: {batch_result.status}\")\n",
    "print(f\"\\nRequest counts:\")\n",
    "print(f\"  Completed: {batch_result.request_counts.completed}\")\n",
    "print(f\"  Failed: {batch_result.request_counts.failed}\")\n",
    "print(f\"  Total: {batch_result.request_counts.total}\")\n",
    "\n",
    "if batch_result.status == \"completed\":\n",
    "    print(f\"\\nBatch completed!\")\n",
    "    print(f\"Output file ID: {batch_result.output_file_id}\")\n",
    "elif batch_result.status in [\"validating\", \"in_progress\", \"finalizing\"]:\n",
    "    print(f\"\\nBatch still processing...\")\n",
    "elif batch_result.status == \"failed\":\n",
    "    print(f\"\\nBatch failed!\")\n",
    "    if batch_result.errors:\n",
    "        print(f\"Errors: {batch_result.errors}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save batch metadata and key_dict for later result retrieval\n",
    "message_batch = batch_result.to_dict()\n",
    "\n",
    "# Build log path\n",
    "log_save_path = LOGS_DIR / INPUT_DIR / MODEL_NAME / f\"{INPUT_FILE}{thinking_suffix}.json\"\n",
    "log_save_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save batch metadata and key mapping\n",
    "saving_dict = {\n",
    "    \"message_batch\": message_batch,\n",
    "    \"key_dict\": key_dict\n",
    "}\n",
    "\n",
    "with open(log_save_path, 'w') as f:\n",
    "    json.dump(saving_dict, f, indent=4, default=str)\n",
    "\n",
    "print(f\"Batch metadata saved to: {log_save_path}\")\n",
    "print(f\"\\nBatch ID: {batch_result.id}\")\n",
    "print(f\"\\nTo check status later, use:\")\n",
    "print(f\"  client.batches.retrieve('{batch_result.id}')\")\n",
    "print(f\"\\nTo retrieve results when complete, use retrieve_batch_results_gpt.ipynb\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
