{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve Batch Results from OpenAI API\n",
    "\n",
    "This notebook retrieves completed batch results from the OpenAI API.\n",
    "\n",
    "## Prerequisites\n",
    "- Set the `OPENAI_API_KEY` environment variable\n",
    "- Have a completed batch job from `create_batch_gpt.ipynb`\n",
    "- Know the path to your batch log file (created during batch submission)\n",
    "\n",
    "## Features\n",
    "- Retrieves results from completed batches\n",
    "- Tracks token usage (including reasoning tokens for o3/GPT-5)\n",
    "- Automatic resubmission of failed prompts\n",
    "- Merges with existing results for incremental collection\n",
    "- Reports errors and missing results\n",
    "\n",
    "## Output Format\n",
    "```json\n",
    "{\n",
    "  \"meta_data\": {\"file_name\": \"...\", \"inference_model\": \"...\"},\n",
    "  \"token_stats\": {\"total_input_tokens\": ..., \"total_output_tokens\": ...},\n",
    "  \"results\": {\"case_001\": \"response text\", ...}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION - Edit these variables before running\n",
    "# ============================================================================\n",
    "from pathlib import Path\n",
    "\n",
    "# Base directory (should match the one used in create_batch_gpt.ipynb)\n",
    "BASE_DIR = Path(\".\")  # Change to your data directory\n",
    "\n",
    "# Settings from batch creation (must match what you used)\n",
    "INPUT_DIR = \"data\"  # Directory that contained input JSON\n",
    "INPUT_FILE = \"your_input_file\"  # Name without .json extension\n",
    "MODEL_NAME = \"gpt-4.1-2025-04-14\"  # Model used for batch\n",
    "REASONING_EFFORT = \"medium\"  # For o3/gpt-5: \"low\", \"medium\", \"high\", or None\n",
    "\n",
    "# Output directory for results\n",
    "OUTPUT_DIR = BASE_DIR / \"output\"\n",
    "LOGS_DIR = BASE_DIR / \"logs\"\n",
    "\n",
    "# ============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "# Determine thinking suffix\n",
    "is_reasoning_model = \"o3\" in MODEL_NAME or \"gpt-5\" in MODEL_NAME\n",
    "thinking_effort = REASONING_EFFORT if is_reasoning_model else None\n",
    "thinking_suffix = f\"_thinking_{thinking_effort}\" if thinking_effort else \"\"\n",
    "\n",
    "# Build paths\n",
    "log_path = LOGS_DIR / INPUT_DIR / MODEL_NAME / f\"{INPUT_FILE}{thinking_suffix}.json\"\n",
    "output_path = OUTPUT_DIR / INPUT_DIR / MODEL_NAME / f\"{INPUT_FILE}{thinking_suffix}.json\"\n",
    "\n",
    "# Load log file to get batch_id and key_dict\n",
    "with open(log_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "key_dict = data[\"key_dict\"]\n",
    "message_batch_id = data['message_batch']['id']\n",
    "\n",
    "print(f\"Log file: {log_path}\")\n",
    "print(f\"Batch ID: {message_batch_id}\")\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Expected results: {len(key_dict)}\")\n",
    "print(f\"Output path: {output_path}\")\n",
    "\n",
    "# Load existing results if any\n",
    "existing_dict = {}\n",
    "if output_path.exists():\n",
    "    with open(output_path, 'r') as f:\n",
    "        existing_dict = json.load(f)\n",
    "    print(f\"Found existing results: {len(existing_dict.get('results', {}))}\")\n",
    "\n",
    "# Initialize client\n",
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_nested_dicts(dict1, dict2):\n",
    "    \"\"\"Merge two dictionaries, keeping existing values on conflict.\"\"\"\n",
    "    result = dict1.copy()\n",
    "    for key, value in dict2.items():\n",
    "        if key not in result:\n",
    "            result[key] = value\n",
    "        elif isinstance(value, dict) and isinstance(result[key], dict):\n",
    "            result[key] = merge_nested_dicts(result[key], value)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we already have all results\n",
    "if existing_dict:\n",
    "    existing_keys = set(existing_dict.get(\"results\", {}).keys())\n",
    "    expected_keys = set(key_dict.values())\n",
    "    \n",
    "    print(f\"Expected results: {len(expected_keys)}\")\n",
    "    print(f\"Existing results: {len(existing_keys)}\")\n",
    "    \n",
    "    if existing_keys == expected_keys:\n",
    "        print(f\"\\nAll results already collected!\")\n",
    "        print(f\"Results file: {output_path}\")\n",
    "        raise SystemExit(\"All results already collected.\")\n",
    "    elif len(existing_keys) > 0:\n",
    "        missing = expected_keys - existing_keys\n",
    "        print(f\"Missing {len(missing)} results from existing file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve batch and results\n",
    "batch = client.batches.retrieve(message_batch_id)\n",
    "print(f\"Batch ID: {batch.id}\")\n",
    "print(f\"Status: {batch.status}\")\n",
    "print(f\"\\nRequest counts:\")\n",
    "print(f\"  Completed: {batch.request_counts.completed}\")\n",
    "print(f\"  Failed: {batch.request_counts.failed}\")\n",
    "print(f\"  Total: {batch.request_counts.total}\")\n",
    "\n",
    "if batch.status != \"completed\":\n",
    "    print(f\"\\nBatch not yet completed. Status: {batch.status}\")\n",
    "    raise SystemExit(\"Batch still processing. Try again later.\")\n",
    "\n",
    "print(\"\\nBatch completed! Retrieving results...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and parse results\n",
    "output_file_id = batch.output_file_id\n",
    "file_response = client.files.content(output_file_id)\n",
    "\n",
    "result_dict = {}\n",
    "processed_ids = set()\n",
    "errors_dict = {}\n",
    "\n",
    "# Token counters\n",
    "total_input_tokens = 0\n",
    "total_output_tokens = 0\n",
    "num_prompts = 0\n",
    "\n",
    "for line in file_response.text.strip().splitlines():\n",
    "    result = json.loads(line)\n",
    "    custom_id = result['custom_id']\n",
    "    processed_ids.add(custom_id)\n",
    "\n",
    "    if result.get(\"error\"):\n",
    "        result_key = key_dict.get(custom_id)\n",
    "        if result_key:\n",
    "            errors_dict[result_key] = result['error']\n",
    "        print(f\"Error for {custom_id}: {result['error']}\")\n",
    "        continue\n",
    "\n",
    "    result_key = key_dict[custom_id]\n",
    "    output = result[\"response\"][\"body\"][\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "    if output:\n",
    "        result_dict[result_key] = output\n",
    "        \n",
    "        # Extract token usage\n",
    "        if \"usage\" in result[\"response\"][\"body\"]:\n",
    "            usage = result[\"response\"][\"body\"][\"usage\"]\n",
    "            total_input_tokens += usage.get(\"prompt_tokens\", 0)\n",
    "            total_output_tokens += usage.get(\"completion_tokens\", 0)\n",
    "            num_prompts += 1\n",
    "\n",
    "# Calculate statistics\n",
    "token_stats = {\n",
    "    \"total_input_tokens\": total_input_tokens,\n",
    "    \"total_output_tokens\": total_output_tokens,\n",
    "    \"num_prompts\": num_prompts,\n",
    "    \"avg_input_tokens\": total_input_tokens / num_prompts if num_prompts > 0 else 0,\n",
    "    \"avg_output_tokens\": total_output_tokens / num_prompts if num_prompts > 0 else 0\n",
    "}\n",
    "\n",
    "# Report results\n",
    "missing_ids = set(key_dict.keys()) - processed_ids\n",
    "print(f\"\\nTotal submitted: {len(key_dict)}\")\n",
    "print(f\"Successful: {len(result_dict)}\")\n",
    "print(f\"Errored: {len(errors_dict)}\")\n",
    "print(f\"Missing: {len(missing_ids)}\")\n",
    "\n",
    "print(f\"\\n=== Token Statistics ===\")\n",
    "print(f\"Total input tokens: {token_stats['total_input_tokens']:,}\")\n",
    "print(f\"Total output tokens: {token_stats['total_output_tokens']:,}\")\n",
    "print(f\"Average input tokens: {token_stats['avg_input_tokens']:.1f}\")\n",
    "print(f\"Average output tokens: {token_stats['avg_output_tokens']:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle failed prompts (optional: automatic resubmission)\n",
    "if missing_ids:\n",
    "    print(f\"\\n{len(missing_ids)} prompts failed or missing.\")\n",
    "    print(f\"Missing custom_ids: {sorted(missing_ids)}\")\n",
    "    print(\"\\nTo resubmit failed prompts:\")\n",
    "    print(\"1. Filter your original input JSON to include only failed cases\")\n",
    "    print(\"2. Run create_batch_gpt.ipynb with the filtered input\")\n",
    "    print(\"3. Retrieve results and merge with existing results\")\n",
    "else:\n",
    "    print(\"\\nAll prompts completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with existing results and save\n",
    "if existing_dict:\n",
    "    result_dict = merge_nested_dicts(existing_dict.get(\"results\", {}), result_dict)\n",
    "    \n",
    "    # Merge token stats\n",
    "    if \"token_stats\" in existing_dict:\n",
    "        old_stats = existing_dict[\"token_stats\"]\n",
    "        token_stats[\"total_input_tokens\"] += old_stats.get(\"total_input_tokens\", 0)\n",
    "        token_stats[\"total_output_tokens\"] += old_stats.get(\"total_output_tokens\", 0)\n",
    "        token_stats[\"num_prompts\"] += old_stats.get(\"num_prompts\", 0)\n",
    "        if token_stats[\"num_prompts\"] > 0:\n",
    "            token_stats[\"avg_input_tokens\"] = token_stats[\"total_input_tokens\"] / token_stats[\"num_prompts\"]\n",
    "            token_stats[\"avg_output_tokens\"] = token_stats[\"total_output_tokens\"] / token_stats[\"num_prompts\"]\n",
    "    print(\"Merged with existing results\")\n",
    "\n",
    "# Create output directory and save\n",
    "output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "final_dict = {\n",
    "    \"meta_data\": {\n",
    "        \"file_name\": INPUT_FILE,\n",
    "        \"inference_model\": MODEL_NAME\n",
    "    },\n",
    "    \"token_stats\": token_stats,\n",
    "    \"results\": result_dict\n",
    "}\n",
    "\n",
    "with open(output_path, \"w\") as f:\n",
    "    json.dump(final_dict, f, indent=4)\n",
    "\n",
    "print(f\"\\nSaved results to: {output_path}\")\n",
    "print(f\"Total cases with results: {len(result_dict)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
