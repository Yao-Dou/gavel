{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Batch Job for Claude API\n",
    "\n",
    "This notebook creates a batch job for processing multiple prompts through the Claude API.\n",
    "\n",
    "## Features\n",
    "- Supports Claude Sonnet 4 and Opus 4 models\n",
    "- Extended thinking mode (add `-thinking` suffix to model name)\n",
    "- Automatic system message handling\n",
    "- Batch metadata saved for result retrieval\n",
    "\n",
    "## Prerequisites\n",
    "- Set the `ANTHROPIC_API_KEY` environment variable\n",
    "- Prepare input JSON file with `keys` and `contexts` arrays\n",
    "\n",
    "## Input Format\n",
    "```json\n",
    "{\n",
    "  \"keys\": [\"case_001\", \"case_002\", ...],\n",
    "  \"contexts\": [\n",
    "    [{\"role\": \"user\", \"content\": \"...\"}],\n",
    "    [{\"role\": \"system\", \"content\": \"...\"}, {\"role\": \"user\", \"content\": \"...\"}],\n",
    "    ...\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "## Output\n",
    "- Batch metadata saved to `logs/{INPUT_DIR}/{MODEL_NAME}/{INPUT_FILE}_thinking_{True/False}.json`\n",
    "- Use `retrieve_batch_results_claude.ipynb` to get results after batch completes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION - Edit these variables before running\n",
    "# ============================================================================\n",
    "from pathlib import Path\n",
    "\n",
    "# Base directory for data files\n",
    "BASE_DIR = Path(\".\")  # Change to your data directory\n",
    "\n",
    "# Input file settings\n",
    "INPUT_DIR = \"data\"  # Directory containing input JSON file\n",
    "INPUT_FILE = \"your_input_file\"  # Name without .json extension\n",
    "\n",
    "# Model settings\n",
    "# Supported models: claude-sonnet-4-20250514, claude-opus-4-1-20250805, claude-opus-4-20250514\n",
    "# Add -thinking suffix to enable extended thinking mode (e.g., claude-sonnet-4-20250514-thinking)\n",
    "MODEL_NAME = \"claude-sonnet-4-20250514-thinking\"\n",
    "\n",
    "# Output directory for logs\n",
    "LOGS_DIR = BASE_DIR / \"logs\"\n",
    "\n",
    "# ============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import anthropic\n",
    "from anthropic.types.message_create_params import MessageCreateParamsNonStreaming\n",
    "from anthropic.types.messages.batch_create_params import Request\n",
    "\n",
    "# Detect thinking mode and get actual model name\n",
    "thinking_enabled = MODEL_NAME.endswith(\"-thinking\")\n",
    "actual_model = MODEL_NAME[:-len(\"-thinking\")] if thinking_enabled else MODEL_NAME\n",
    "thinking_budget = 16000 if thinking_enabled else None\n",
    "\n",
    "# Supported Claude 4 models (don't use top_p with these)\n",
    "CLAUDE_4_MODELS = [\"claude-sonnet-4-20250514\", \"claude-opus-4-1-20250805\", \"claude-opus-4-20250514\"]\n",
    "\n",
    "# Initialize Anthropic client\n",
    "client = anthropic.Anthropic(\n",
    "    api_key=os.environ.get(\"ANTHROPIC_API_KEY\"),\n",
    ")\n",
    "\n",
    "# Build input path\n",
    "input_path = BASE_DIR / INPUT_DIR / f\"{INPUT_FILE}.json\"\n",
    "\n",
    "# Load the input prompt data\n",
    "with open(input_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(f\"Input file: {input_path}\")\n",
    "print(f\"Loaded {len(data['keys'])} cases\")\n",
    "print(f\"Model: {actual_model}\")\n",
    "print(f\"Thinking enabled: {thinking_enabled}\")\n",
    "if thinking_enabled:\n",
    "    print(f\"Thinking budget: {thinking_budget} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build list of Request objects for batch API\n",
    "# NOTE: Unlike OpenAI, Anthropic accepts requests directly - no JSONL file upload needed!\n",
    "\n",
    "requests = []\n",
    "key_dict = {}\n",
    "\n",
    "for i, (key, context) in enumerate(list(zip(data[\"keys\"], data[\"contexts\"]))):\n",
    "    # Handle system message\n",
    "    messages = context.copy()\n",
    "    system_content = None\n",
    "    \n",
    "    if messages and messages[0][\"role\"] == \"system\":\n",
    "        # Extract system message and remove from messages array\n",
    "        system_content = messages[0][\"content\"]\n",
    "        messages = messages[1:]\n",
    "    \n",
    "    # Build params dict for MessageCreateParamsNonStreaming\n",
    "    params_dict = {\n",
    "        \"model\": actual_model,\n",
    "        \"max_tokens\": 20000,\n",
    "        \"temperature\": 1.0 if thinking_enabled else 0.7,\n",
    "        \"messages\": messages,\n",
    "    }\n",
    "    \n",
    "    # Add system parameter if system message exists\n",
    "    if system_content:\n",
    "        params_dict[\"system\"] = system_content\n",
    "    \n",
    "    # Add thinking parameter if enabled\n",
    "    if thinking_enabled:\n",
    "        params_dict[\"thinking\"] = {\n",
    "            \"type\": \"enabled\",\n",
    "            \"budget_tokens\": thinking_budget\n",
    "        }\n",
    "    \n",
    "    # Create Request object\n",
    "    batch_request = Request(\n",
    "        custom_id=str(i),\n",
    "        params=MessageCreateParamsNonStreaming(**params_dict)\n",
    "    )\n",
    "    \n",
    "    key_dict[str(i)] = key\n",
    "    requests.append(batch_request)\n",
    "\n",
    "print(f\"Created {len(requests)} batch requests\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the batch - pass requests directly (no file upload!)\n",
    "message_batch = client.messages.batches.create(\n",
    "    requests=requests\n",
    ")\n",
    "\n",
    "print(f\"Batch created successfully!\")\n",
    "print(f\"Batch ID: {message_batch.id}\")\n",
    "print(f\"Status: {message_batch.processing_status}\")\n",
    "print(f\"\\nFull batch info:\")\n",
    "print(message_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check batch status (re-run this cell to check progress)\n",
    "message_batch = client.messages.batches.retrieve(message_batch.id)\n",
    "\n",
    "print(f\"Batch ID: {message_batch.id}\")\n",
    "print(f\"Status: {message_batch.processing_status}\")\n",
    "print(f\"\\nRequest counts:\")\n",
    "print(f\"  Processing: {message_batch.request_counts.processing}\")\n",
    "print(f\"  Succeeded: {message_batch.request_counts.succeeded}\")\n",
    "print(f\"  Errored: {message_batch.request_counts.errored}\")\n",
    "print(f\"  Canceled: {message_batch.request_counts.canceled}\")\n",
    "print(f\"  Expired: {message_batch.request_counts.expired}\")\n",
    "\n",
    "if message_batch.processing_status == \"ended\":\n",
    "    print(f\"\\nBatch completed!\")\n",
    "    if message_batch.results_url:\n",
    "        print(f\"Results URL: {message_batch.results_url}\")\n",
    "elif message_batch.processing_status == \"in_progress\":\n",
    "    print(f\"\\nBatch still processing...\")\n",
    "else:\n",
    "    print(f\"\\nStatus: {message_batch.processing_status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save batch metadata and key_dict for later result retrieval\n",
    "\n",
    "# Convert batch object to dict\n",
    "try:\n",
    "    message_batch_dict = message_batch.to_dict()\n",
    "except AttributeError:\n",
    "    message_batch_dict = dict(message_batch)\n",
    "\n",
    "# Build log save path\n",
    "thinking_suffix = \"_thinking_True\" if thinking_enabled else \"\"\n",
    "log_save_path = LOGS_DIR / INPUT_DIR / MODEL_NAME / f\"{INPUT_FILE}{thinking_suffix}.json\"\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "log_save_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save batch metadata and key mapping\n",
    "saving_dict = {\n",
    "    \"message_batch\": message_batch_dict,\n",
    "    \"key_dict\": key_dict\n",
    "}\n",
    "\n",
    "with open(log_save_path, 'w') as f:\n",
    "    json.dump(saving_dict, f, indent=4, default=str)\n",
    "\n",
    "print(f\"Batch metadata saved to: {log_save_path}\")\n",
    "print(f\"\\nBatch ID: {message_batch.id}\")\n",
    "print(f\"\\nTo check status later, use:\")\n",
    "print(f\"  client.messages.batches.retrieve('{message_batch.id}')\")\n",
    "print(f\"\\nTo retrieve results when complete, use retrieve_batch_results_claude.ipynb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: List recent batches\n",
    "print(\"Recent batches:\\n\")\n",
    "for i, batch in enumerate(client.messages.batches.list(limit=10)):\n",
    "    print(f\"{i+1}. ID: {batch.id}\")\n",
    "    print(f\"   Status: {batch.processing_status}\")\n",
    "    print(f\"   Created: {batch.created_at}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
