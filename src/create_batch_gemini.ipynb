{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Batch Job for Gemini API\n",
    "\n",
    "This notebook creates a batch job for processing multiple prompts through the Google Gemini API.\n",
    "\n",
    "## Features\n",
    "- Supports Gemini 2.5 (Pro, Flash) and Gemini 3 models\n",
    "- Configurable thinking mode:\n",
    "  - Gemini 2.5: `thinkingBudget` (-1 for dynamic, or specific token count)\n",
    "  - Gemini 3: `thinkingLevel` (\"low\" or \"high\")\n",
    "- Automatic OpenAI-to-Gemini message format conversion\n",
    "- Batch metadata saved for result retrieval\n",
    "\n",
    "## Prerequisites\n",
    "- Set the `GEMINI_API_KEY` environment variable\n",
    "- Prepare input JSON file with `keys` and `contexts` arrays\n",
    "\n",
    "## Input Format\n",
    "```json\n",
    "{\n",
    "  \"keys\": [\"case_001\", \"case_002\", ...],\n",
    "  \"contexts\": [\n",
    "    [{\"role\": \"user\", \"content\": \"...\"}],\n",
    "    [{\"role\": \"system\", \"content\": \"...\"}, {\"role\": \"user\", \"content\": \"...\"}],\n",
    "    ...\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "## Output\n",
    "- JSONL file uploaded to Gemini File API\n",
    "- Batch metadata saved to `logs/{INPUT_DIR}/{MODEL_NAME}/{INPUT_FILE}.json`\n",
    "- Use `retrieve_batch_results_gemini.ipynb` to get results after batch completes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION - Edit these variables before running\n",
    "# ============================================================================\n",
    "from pathlib import Path\n",
    "\n",
    "# Base directory for data files\n",
    "BASE_DIR = Path(\".\")  # Change to your data directory\n",
    "\n",
    "# Input file settings\n",
    "INPUT_DIR = \"data\"  # Directory containing input JSON file\n",
    "INPUT_FILE = \"your_input_file\"  # Name without .json extension\n",
    "\n",
    "# Model settings\n",
    "# Supported Gemini 2.5: gemini-2.5-pro, gemini-2.5-flash\n",
    "# Supported Gemini 3: gemini-3-pro-preview\n",
    "MODEL_NAME = \"gemini-2.5-pro\"\n",
    "\n",
    "# Thinking configuration\n",
    "# For Gemini 2.5: Set THINKING_BUDGET (-1 for dynamic, or specific token count)\n",
    "# For Gemini 3: Set THINKING_LEVEL (\"low\" or \"high\")\n",
    "THINKING_LEVEL = \"high\"   # Gemini 3 only\n",
    "THINKING_BUDGET = -1      # Gemini 2.5 only (-1 = dynamic)\n",
    "\n",
    "# Output directories\n",
    "LOGS_DIR = BASE_DIR / \"logs\"\n",
    "JSONL_DIR = BASE_DIR / \"gemini_jsonl\"\n",
    "\n",
    "# ============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "# Detect model version for thinking config\n",
    "is_gemini_3 = \"gemini-3\" in MODEL_NAME\n",
    "\n",
    "# Initialize Gemini client\n",
    "client = genai.Client(api_key=os.environ.get(\"GEMINI_API_KEY\"))\n",
    "\n",
    "# Build paths\n",
    "input_path = BASE_DIR / INPUT_DIR / f\"{INPUT_FILE}.json\"\n",
    "jsonl_path = JSONL_DIR / INPUT_DIR / MODEL_NAME / f\"{INPUT_FILE}.jsonl\"\n",
    "jsonl_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load input data\n",
    "with open(input_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(f\"Input file: {input_path}\")\n",
    "print(f\"Loaded {len(data['keys'])} cases\")\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Gemini 3: {is_gemini_3}\")\n",
    "if is_gemini_3:\n",
    "    print(f\"Thinking level: {THINKING_LEVEL}\")\n",
    "else:\n",
    "    print(f\"Thinking budget: {THINKING_BUDGET}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build JSONL batch requests\n",
    "requests = []\n",
    "key_dict = {}\n",
    "\n",
    "for i, (key, context) in enumerate(list(zip(data[\"keys\"], data[\"contexts\"]))):\n",
    "    # Handle system instruction\n",
    "    system_instruction = None\n",
    "    messages = context.copy()\n",
    "    \n",
    "    if messages and messages[0][\"role\"] == \"system\":\n",
    "        system_content = messages[0][\"content\"]\n",
    "        system_instruction = {\n",
    "            \"parts\": [{\"text\": system_content}]\n",
    "        }\n",
    "        messages = messages[1:]  # Remove system from messages\n",
    "    \n",
    "    # Convert OpenAI-style messages to Gemini contents format\n",
    "    contents = []\n",
    "    for message in messages:\n",
    "        # Map roles: user -> user, assistant -> model\n",
    "        role = \"user\" if message[\"role\"] == \"user\" else \"model\"\n",
    "        contents.append({\n",
    "            \"role\": role,\n",
    "            \"parts\": [{\"text\": message[\"content\"]}]\n",
    "        })\n",
    "    \n",
    "    # Build generationConfig\n",
    "    generation_config = {\n",
    "        \"temperature\": 0.7,\n",
    "        \"topP\": 1.0,\n",
    "        \"maxOutputTokens\": 20000,\n",
    "    }\n",
    "    \n",
    "    # Add thinking config based on model version\n",
    "    if is_gemini_3:\n",
    "        generation_config[\"thinkingConfig\"] = {\"thinkingLevel\": THINKING_LEVEL}\n",
    "    else:\n",
    "        generation_config[\"thinkingConfig\"] = {\"thinkingBudget\": THINKING_BUDGET}\n",
    "    \n",
    "    # Build batch request\n",
    "    batch_request = {\n",
    "        \"key\": str(i),\n",
    "        \"request\": {\n",
    "            \"contents\": contents,\n",
    "            \"generationConfig\": generation_config\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Add system instruction if exists\n",
    "    if system_instruction:\n",
    "        batch_request[\"request\"][\"systemInstruction\"] = system_instruction\n",
    "    \n",
    "    key_dict[str(i)] = key\n",
    "    requests.append(batch_request)\n",
    "\n",
    "# Write to JSONL file\n",
    "with open(jsonl_path, \"w\") as f:\n",
    "    for req in requests:\n",
    "        json.dump(req, f)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(f\"Created {len(requests)} batch requests\")\n",
    "print(f\"Saved to: {jsonl_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload JSONL file to Gemini File API\n",
    "uploaded_file = client.files.upload(\n",
    "    file=str(jsonl_path),\n",
    "    config=types.UploadFileConfig(\n",
    "        display_name=INPUT_FILE,\n",
    "        mime_type='application/json'\n",
    "    )\n",
    ")\n",
    "\n",
    "print(f\"Uploaded file:\")\n",
    "print(f\"  Name: {uploaded_file.name}\")\n",
    "print(f\"  Display name: {uploaded_file.display_name}\")\n",
    "print(f\"  URI: {uploaded_file.uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create batch job\n",
    "file_batch_job = client.batches.create(\n",
    "    model=MODEL_NAME,\n",
    "    src=uploaded_file.name,\n",
    "    config={\n",
    "        'display_name': f\"{MODEL_NAME}_{INPUT_FILE}\",\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"Created batch job:\")\n",
    "print(f\"  Name: {file_batch_job.name}\")\n",
    "print(f\"  State: {file_batch_job.state.name}\")\n",
    "print(f\"  Model: {file_batch_job.model}\")\n",
    "print(f\"  Create time: {file_batch_job.create_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check batch status (re-run this cell to check progress)\n",
    "batch_job = client.batches.get(name=file_batch_job.name)\n",
    "\n",
    "print(f\"Batch job: {batch_job.name}\")\n",
    "print(f\"State: {batch_job.state.name}\")\n",
    "\n",
    "# Possible states:\n",
    "# JOB_STATE_PENDING, JOB_STATE_RUNNING, JOB_STATE_SUCCEEDED,\n",
    "# JOB_STATE_FAILED, JOB_STATE_CANCELLED, JOB_STATE_EXPIRED\n",
    "\n",
    "if batch_job.state.name == \"JOB_STATE_SUCCEEDED\":\n",
    "    print(f\"\\nBatch completed!\")\n",
    "    if batch_job.dest and batch_job.dest.file_name:\n",
    "        print(f\"Results file: {batch_job.dest.file_name}\")\n",
    "    elif batch_job.dest and batch_job.dest.inlined_responses:\n",
    "        print(f\"Results are inline (count: {len(batch_job.dest.inlined_responses)})\")\n",
    "    else:\n",
    "        print(\"Results location unknown\")\n",
    "elif batch_job.state.name in [\"JOB_STATE_PENDING\", \"JOB_STATE_RUNNING\"]:\n",
    "    print(f\"\\nBatch still processing...\")\n",
    "elif batch_job.state.name == \"JOB_STATE_FAILED\":\n",
    "    print(f\"\\nBatch failed!\")\n",
    "    if hasattr(batch_job, 'error') and batch_job.error:\n",
    "        print(f\"Error: {batch_job.error}\")\n",
    "elif batch_job.state.name == \"JOB_STATE_CANCELLED\":\n",
    "    print(f\"\\nBatch was cancelled\")\n",
    "elif batch_job.state.name == \"JOB_STATE_EXPIRED\":\n",
    "    print(f\"\\nBatch expired (exceeded 48-hour window)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save batch metadata and key_dict for later result retrieval\n",
    "\n",
    "# Convert batch job to dict for saving\n",
    "try:\n",
    "    batch_job_dict = batch_job.to_dict()\n",
    "except AttributeError:\n",
    "    batch_job_dict = {\n",
    "        \"name\": batch_job.name,\n",
    "        \"model\": batch_job.model,\n",
    "        \"state\": batch_job.state.name,\n",
    "        \"create_time\": str(batch_job.create_time),\n",
    "    }\n",
    "    if hasattr(batch_job, 'dest') and batch_job.dest:\n",
    "        if hasattr(batch_job.dest, 'file_name') and batch_job.dest.file_name:\n",
    "            batch_job_dict[\"dest_file_name\"] = batch_job.dest.file_name\n",
    "\n",
    "# Build log path\n",
    "log_save_path = LOGS_DIR / INPUT_DIR / MODEL_NAME / f\"{INPUT_FILE}.json\"\n",
    "log_save_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save batch metadata and key mapping\n",
    "saving_dict = {\n",
    "    \"batch_job\": batch_job_dict,\n",
    "    \"key_dict\": key_dict,\n",
    "    \"uploaded_file\": {\n",
    "        \"name\": uploaded_file.name,\n",
    "        \"uri\": uploaded_file.uri\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(log_save_path, 'w') as f:\n",
    "    json.dump(saving_dict, f, indent=4, default=str)\n",
    "\n",
    "print(f\"Batch metadata saved to: {log_save_path}\")\n",
    "print(f\"\\nBatch job name: {file_batch_job.name}\")\n",
    "print(f\"\\nTo check status later, use:\")\n",
    "print(f\"  client.batches.get(name='{file_batch_job.name}')\")\n",
    "print(f\"\\nTo retrieve results when complete, use retrieve_batch_results_gemini.ipynb\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
